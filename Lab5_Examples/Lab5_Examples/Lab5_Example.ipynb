{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c542647",
   "metadata": {},
   "source": [
    "# <span style='color:blue'> LAB 5: </span>\n",
    "# <span style='color:blue'> RECURRENT NEURAL NETWORKS </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d050509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c7674",
   "metadata": {},
   "source": [
    "## <span style='color:red'> Part 4: RNN Implementation in PyTorch </span>\n",
    "## <span style='color:red'> Character Level Text Generation using Shakespeare Dataset) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f149d33a",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0844f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 10000 characters, 57 unique\n"
     ]
    }
   ],
   "source": [
    "# First n-characters to use for training\n",
    "data_size_to_train = 10000\n",
    "\n",
    "# Load the Shakespeare data up to data_size_to_train\n",
    "data = open('shakespeare.txt', 'r').read()[:data_size_to_train]\n",
    "\n",
    "# Find the unique characters within the training data\n",
    "characters = sorted(list(set(data)))\n",
    "\n",
    "# total number of characters in the training data and number of unique characters\n",
    "data_size, vocab_size = len(data), len(characters)\n",
    "\n",
    "print(\"Data has {} characters, {} unique\".format(data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b647070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Python Dictionary to map the characters to numbers and vice versa\n",
    "\n",
    "character_to_num = { ch:i for i,ch in enumerate(characters) }\n",
    "num_to_character = { i:ch for i,ch in enumerate(characters) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33bcbf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, ',': 4, '-': 5, '.': 6, ':': 7, ';': 8, '?': 9, 'A': 10, 'B': 11, 'C': 12, 'D': 13, 'E': 14, 'F': 15, 'H': 16, 'I': 17, 'J': 18, 'L': 19, 'M': 20, 'N': 21, 'O': 22, 'P': 23, 'R': 24, 'S': 25, 'T': 26, 'U': 27, 'V': 28, 'W': 29, 'Y': 30, 'a': 31, 'b': 32, 'c': 33, 'd': 34, 'e': 35, 'f': 36, 'g': 37, 'h': 38, 'i': 39, 'j': 40, 'k': 41, 'l': 42, 'm': 43, 'n': 44, 'o': 45, 'p': 46, 'q': 47, 'r': 48, 's': 49, 't': 50, 'u': 51, 'v': 52, 'w': 53, 'x': 54, 'y': 55, 'z': 56}\n"
     ]
    }
   ],
   "source": [
    "print(character_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1300d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the character_to_num dictionary to map each character in the training dataset to number\n",
    "\n",
    "data = list(data)\n",
    "\n",
    "for i, ch in enumerate(data):\n",
    "    data[i] = character_to_num[ch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eacbda6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 39, 48, 49, 50, 1, 12, 39, 50, 39]\n"
     ]
    }
   ],
   "source": [
    "# Print first 10 characters represented as numbers \n",
    "\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe7863",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d29ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embeddings, embedding_dim, input_size, hidden_size, num_layers, output_size):\n",
    "        \n",
    "        super(CharRNN, self).__init__()\n",
    "        \n",
    "        # Define embedding layer\n",
    "        # num_embeddings = number of unique characters\n",
    "        # embedding_dim = size of a vector that encodes each character\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings, embedding_dim)\n",
    "        \n",
    "        # Define vanilla RNN cell\n",
    "        # num_layers = Number of RNN cells to be stacked for deep-RNN\n",
    "        # nonlinearity = Activation function to use\n",
    "        self.rnn = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size, \n",
    "                                num_layers=num_layers, \n",
    "                                nonlinearity = 'relu')\n",
    "        \n",
    "        # decoder layer that takes hidden states as inputs and output probabilities for each character\n",
    "        # output_size = number of unique characters\n",
    "        self.decoder = torch.nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        \n",
    "        # Forward pass input sequence to embedding layer \n",
    "        embedding = self.embedding(input_seq)\n",
    "        \n",
    "        # RNN cell takes output of embedding layer + previous hidden state as inputs\n",
    "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
    "        \n",
    "        # Forward pass the RNN cell output to decoder to get the probabilities\n",
    "        output = self.decoder(output)\n",
    "        \n",
    "        # hidden states need to be detached from computation graph to be re-used as input\n",
    "        return output, hidden_state.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cac3a5",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c001665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (embedding): Embedding(57, 100)\n",
       "  (rnn): RNN(100, 512, num_layers=3)\n",
       "  (decoder): Linear(in_features=512, out_features=57, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix random seed\n",
    "torch.manual_seed(25)\n",
    "\n",
    "# Define RNN network\n",
    "rnn = CharRNN(num_embeddings = vocab_size, embedding_dim = 100, \n",
    "              input_size = 100, hidden_size = 512, num_layers = 3,\n",
    "              output_size = vocab_size)\n",
    "\n",
    "# Define learning rate and epochs\n",
    "learning_rate = 0.001          \n",
    "epochs = 50\n",
    "\n",
    "# Size of the input sequence to be used during training and validation\n",
    "# Note that too long input sequence can lead to unstable training via exploding/vanishing gradients\n",
    "training_sequence_len = 50\n",
    "validation_sequence_len = 200    \n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# add .cuda() for GPU acceleration\n",
    "rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b88a3d",
   "metadata": {},
   "source": [
    "## Identify Tracked Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a9efa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking training loss per each input/target sequence fwd/bwd pass\n",
    "train_loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b818cf8",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a812d26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged Training Loss for Epoch  0 :  2.5955934344823635\n",
      "----------------------------------------\n",
      "e wegeer anmuaunhgatueor'E\n",
      "SeaI that that'ds\n",
      "'nd mid yuswrtrrar Cy\n",
      "Sukrees-\n",
      "M\n",
      "Souse,\n",
      "Theur!\n",
      "They ass Whou weind nop therrp':\n",
      "Ther, munubudl:\n",
      "They it kebken:\n",
      "Thoufw'ry tor fey sutx oo- the rein toud me\n",
      "----------------------------------------\n",
      "Averaged Training Loss for Epoch  1 :  2.066283651373603\n",
      "----------------------------------------\n",
      "Andrlat strong benimus -e?\n",
      "\n",
      "MENEIUS:\n",
      "Frartics tiun'inee; mus inny fork; blab! not my picf Cofn hus?\n",
      "\n",
      "MENENIUS:\n",
      "Yhat not prop, oor theik sob your Roir deoue Cacfuee,\n",
      "Whet,\n",
      "Afh'r your strear, sas vatgtW\n",
      "----------------------------------------\n",
      "Averaged Training Loss for Epoch  2 :  1.8395616273687343\n",
      "----------------------------------------\n",
      "o dist onile who dest:\n",
      "Shour com deif they man.\n",
      "\n",
      "MARCIUS:\n",
      "That theone?\n",
      "\n",
      "MARCIUS:\n",
      "Corlm us,\n",
      "Ond fees, ou lase talitged\n",
      "-rom the god, as the cain,\n",
      "Wat and brould: are hang in the mest twer hakes stet. Y\n",
      "----------------------------------------\n",
      "Averaged Training Loss for Epoch  3 :  1.6708471487514938\n",
      "----------------------------------------\n",
      "r man,\n",
      "Their liig:\n",
      "And,\n",
      "As their osh mep, theer.\n",
      "\n",
      "MARCIUS:\n",
      " Maplecing would sutoudimed the ow!\n",
      "Thain aid pood their make god;\n",
      "Wh you answ not they thouve gakeso thusided,\n",
      "Bed are degowiciniciong\n",
      "Our o\n",
      "----------------------------------------\n",
      "Averaged Training Loss for Epoch  4 :  1.539803598264251\n",
      "----------------------------------------\n",
      "s not they, thee uo,\n",
      "and it the bependt to degend them,\n",
      "The oguld fondy helved,\n",
      "There\n",
      "A magund a suapoud\n",
      "-y not\n",
      "And fend thelbd,\n",
      "Yet thn keught thees to\n",
      " of gaid\n",
      "word;\n",
      "They thnong\n",
      "As fortu good thob-'\n",
      "----------------------------------------\n",
      "Averaged Training Loss for Epoch  5 :  1.4317593709308298\n",
      "----------------------------------------\n",
      " lokes; side in hand they thour sto' th mone them fire, poon th kttheck make bodid premulsered are an th break sabbection.\n",
      "\n",
      "MENENIUS:\n",
      "hat mest the coknot Capinoused the kisions, the pecouds one hores \n",
      "----------------------------------------\n",
      "Averaged Training Loss for Epoch  6 :  1.315823234994002\n",
      "----------------------------------------\n",
      "ow hang 'emies to dedenge the Ronged you brere's tone. 'y lave hing hang they must thed vemblrtest know the moon,\n",
      "Five trese chat mest partur powned memmat meved!\n",
      "\n",
      "MARCIUS:\n",
      "The moon\n",
      "\n",
      "MARCIUS:\n",
      "For corb\n",
      "----------------------------------------\n",
      "Averaged Training Loss for Epoch  7 :  1.2211233416591027\n",
      "----------------------------------------\n",
      "rey evord cans heral bell other to these good comn our, they latk being awe these sthang 'im firk enoug, th theow, Iusim ange; of they shall wese an\n",
      "They gacoust the well other luk, theJet, that mutt \n",
      "----------------------------------------\n",
      "Averaged Training Loss for Epoch  8 :  1.1004681335856217\n",
      "----------------------------------------\n",
      "us\n",
      "ing\n",
      "As them waik the weiliog,\n",
      "Under the rianted their own they lack munh aguni'g being their own pattly marrimane anoth rethe word, partion,\n",
      "That mept well; Autime them out dicd their pitulang 'en?\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Empty gradient buffer -> backpropagation -> update network\u001b[39;00m\n\u001b[0;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 36\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Update starting character for next sequence\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jeff\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jeff\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jeff\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert training data into torch tensor and make it into vertical orientation (N, 1)\n",
    "# Attach .cuda() if using GPU\n",
    "data = torch.unsqueeze(torch.tensor(data), dim = 1)\n",
    "\n",
    "# Training Loop ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Randomly select a starting character from first 100 characters in training set\n",
    "    character_loc = np.random.randint(100)\n",
    "    \n",
    "    # iteration number to keep track of until the sequence reaches the end of training data\n",
    "    iteration = 0\n",
    "    \n",
    "    # initialize initial hiddens state as None\n",
    "    hidden_state = None\n",
    "\n",
    "    while character_loc + training_sequence_len + 1 < data_size: # loop continues until target_seq reaches end of the data\n",
    "        \n",
    "        # Define input/target sequence\n",
    "        input_seq = data[character_loc : character_loc + training_sequence_len]\n",
    "        target_seq = data[character_loc + 1 : character_loc + training_sequence_len + 1]\n",
    "        \n",
    "        # Pass input sequence and hidden_state to RNN\n",
    "        output, hidden_state = rnn(input_seq, hidden_state)\n",
    "        \n",
    "        # Compute loss between RNN output sequence vs target sequence\n",
    "        # torch.squeeze removes the column dimension and make them into horizontal orientation\n",
    "        loss = loss_fn(torch.squeeze(output), torch.squeeze(target_seq))\n",
    "        \n",
    "        # Append loss\n",
    "        train_loss_list.append(loss.item())\n",
    "        \n",
    "        # Empty gradient buffer -> backpropagation -> update network\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update starting character for next sequence\n",
    "        character_loc += training_sequence_len\n",
    "        \n",
    "        # Update iteration number\n",
    "        iteration += 1\n",
    "            \n",
    "    print(\"Averaged Training Loss for Epoch \", epoch,\": \", np.mean(train_loss_list[-iteration:]))\n",
    "    \n",
    "    # Sample and generate a text sequence after every epoch --------------------------------------------------------------\n",
    "    \n",
    "    #Initialize character location and hidden state for validation\n",
    "    character_loc = 0\n",
    "    hidden_state = None\n",
    "    \n",
    "    # Pick a random character from the dataset as an initial input to RNN \n",
    "    rand_index = np.random.randint(data_size-1)\n",
    "    input_seq = data[rand_index : rand_index+1]\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        while character_loc < validation_sequence_len: # Loop continues until RNN generated sequence is in desired length\n",
    "            \n",
    "            # Pass validation sequence to RNN\n",
    "            # Note that RNN now uses its previous output character as input\n",
    "            output, hidden_state = rnn(input_seq, hidden_state)\n",
    "            \n",
    "            # Take the softmax of the decoder output to get the probabilities of predicted characters\n",
    "            output = torch.nn.functional.softmax(torch.squeeze(output), dim=0)\n",
    "            # Use the  probabilities to sample the output character\n",
    "            character_distribution = torch.distributions.Categorical(output)\n",
    "            character_num = character_distribution.sample()\n",
    "            \n",
    "            # Convert the character number selected from sampling to actual character and print\n",
    "            print(num_to_character[character_num.item()], end='')\n",
    "            \n",
    "            # Update the input_seq so that it's using the output of the RNN as new input\n",
    "            input_seq[0][0] = character_num.item()\n",
    "            \n",
    "            # Update the character location \n",
    "            character_loc += 1\n",
    "\n",
    "    print(\"\\n----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0442adb6",
   "metadata": {},
   "source": [
    "## Visualize & Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cfc511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn for prettier plot\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style = 'whitegrid', font_scale = 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss and rolling mean training loss with respect to iterations\n",
    "plt.figure(figsize = (15, 9))\n",
    "\n",
    "plt.plot(train_loss_list, linewidth = 3, label = 'Training Loss')\n",
    "plt.plot(np.convolve(train_loss_list, np.ones(100), 'valid') / 100, \n",
    "         linewidth = 3, label = 'Rolling Averaged Training Loss')\n",
    "plt.ylabel(\"training loss\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
